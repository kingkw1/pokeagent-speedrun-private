# AI Agent for the Pok√©Agent Speedrunning Challenge

This repository contains the development of a sophisticated, hierarchical AI agent designed to compete in **Track 2 (RPG Speedrunning)** of the NeurIPS 2025 Pok√©Agent Challenge. Our goal is to build a high-performance agent that can complete Pok√©mon Emerald by optimizing for the competition's primary ranking metric: **Raw Performance** (milestone completion percentage, time, and action efficiency). Our innovative architecture also aims for the **Judges' Choice** awards by demonstrating novel AI capabilities.

This project is built upon the official starter kit but features a custom, learning-driven architecture with advanced perception capabilities and intelligent action sequencing.

---

## Current Status & Recent Achievements

**üéâ Major Breakthrough: Fixed the "Brain Confusion" Problem (October 11, 2025)**

We've successfully resolved a critical issue where the VLM perception system was returning invalid JSON and the action system was "button mashing" with all possible actions. The agent now makes intelligent, context-aware decisions:

- ‚úÖ **VLM Perception Fixed**: Implemented robust JSON parsing with Python tuple to JSON array conversion
- ‚úÖ **Smart Action Sequencing**: Agent now makes single intelligent actions or controlled sequences for navigation
- ‚úÖ **Real-Time Performance**: Agent runs at ~60 FPS with 2.3s VLM inference time using Qwen2-VL-2B-Instruct
- ‚úÖ **Intelligent Behavior**: Agent properly advances through dialogue, navigates menus, and explores the overworld

## Architectural Overview

This agent is built upon the starter kit's four-module framework (Perception, Planning, Memory, Action) but implements a sophisticated, learning-driven architecture designed to maximize autonomy and performance. Our design's core philosophy is to replace hard-coded components with learned policies to minimize the competition's scaffolding penalty.

### 1. Hierarchical Command Structure: Strategic Planner & Tactical Controller
To address the long-horizon challenge of an RPG, our agent uses a two-layer hierarchical command structure that separates high-level strategy from low-level execution.

* **High-Level Planner:** The strategic brain of the agent will be a fine-tuned Large Language Model (LLM). It is trained to analyze the game state and memory to issue the next major subgoal, such as `{"subgoal": "NAVIGATE_TO", "target": "Pewter City Gym"}`. This allows the agent to reason about the critical path of the speedrun at a strategic level.
* **Low-Level Controller:** A goal-conditioned reinforcement learning (RL) policy is responsible for tactical execution. It takes a subgoal from the planner (e.g., "NAVIGATE_TO") and outputs the sequence of primitive game actions required to achieve it efficiently.

### 2. Advanced VLM Perception: Image-to-Structure Translation
The agent's "vision" is powered by a fine-tuned Vision-Language Model (VLM). Our perception module moves beyond simple descriptions by performing **image-to-structure translation**. Given a raw game screenshot, the VLM is trained to output a structured JSON object representing the complete multi-modal game state. This provides rich, machine-readable data for all other modules, forming a robust foundation for decision-making.

### 3. Active Memory Management
To handle a game spanning thousands of steps, our blueprint specifies a hybrid memory system that is actively managed by a dedicated RL agent, the Memory Management Agent (MMA).

* **Hybrid Memory:** The system includes a short-term "scratchpad," a long-term episodic memory (Vector DB), and a structured semantic memory (Knowledge Graph).
* **Memory Management Agent (MMA):** This RL agent learns a policy for what information to store, what to retrieve, and what to forget. By learning to make goal-oriented memory decisions, the MMA transforms memory from a passive database into an active component of the agent's reasoning process, a key innovation for minimizing the scaffolding penalty.

### 4. Phased Training Protocol
The agent's complex capabilities are built incrementally through a structured, four-phase training curriculum. This approach de-risks development by mastering foundational skills like perception and tactical execution before moving on to more complex strategic planning and memory management.

---

## Development Progress

**Week 1 COMPLETED (October 9-11, 2025): The Skeleton Agent**

- ‚úÖ **Environment & Baseline**: Successfully set up complete development environment and validated baseline agent
- ‚úÖ **VLM Integration**: Integrated Qwen2-VL-2B-Instruct model with 16x performance improvement over initial models
- ‚úÖ **End-to-End Agent Loop**: Built stable perceive-plan-act loop that runs without crashing
- ‚úÖ **Perception System**: Implemented structured JSON extraction with robust error handling
- ‚úÖ **Action System**: Created intelligent action sequencing with context-aware decision making
- ‚úÖ **Critical Bug Fixes**: Resolved "brain confusion" issues with VLM JSON parsing and action selection

**Current Status**: Agent is now making intelligent decisions and progressing through Pokemon Emerald's intro sequence. Ready for Week 2 development focusing on strategic planning and navigation optimization.

---

## Key Features

This agent implements several innovative features beyond the baseline starter kit:

- **üîç Advanced Visual Perception**: Fine-tuned Qwen2-VL model that converts game screens to structured JSON data
- **üß† Intelligent Action Sequencing**: Context-aware decision making with controlled multi-action sequences
- **‚ö° Real-Time Performance**: 2.3s VLM inference with ~60 FPS game execution
- **üõ°Ô∏è Robust Error Handling**: Fallback systems and error recovery for stable operation
- **üìä Structured State Representation**: Rich semantic understanding of game state beyond pixel data
- **üéØ Context-Aware Planning**: Decisions adapt based on current screen context (dialogue, menu, overworld)
- **üîß Modular Architecture**: Clean separation of Perception, Planning, Memory, and Action modules

---

## Installation

### Prerequisites

- Python 3.10+ with CUDA-compatible GPU recommended
- `mgba` system library for game emulation
- Legally obtained Pokemon Emerald ROM

### Setup Steps

1. **Clone the Repository**
```bash
git clone <your-repo-url>
cd pokeagent-speedrun
```

2. **Install Dependencies**
```bash
# Install uv package manager if not already installed
curl -LsSf https://astral.sh/uv/install.sh | sh

# Create virtual environment and install all dependencies
uv sync
```

3. **Install System Dependencies**
```bash
# Ubuntu/Debian
sudo apt-get install libmgba-dev

# macOS (with Homebrew)
brew install mgba
```

4. **Add Pokemon Emerald ROM**
Place your legally obtained `rom.gba` file in the `Emerald-GBAdvance/` directory.

For detailed setup instructions, see `docs/STARTERKIT_README.md`.

---

## Running the Agent

### Quick Start

```bash
# Activate the virtual environment
source .venv/bin/activate

# Run the agent in autonomous mode
python run.py --agent-auto
```

This launches Pokemon Emerald and starts the AI agent, which will:
1. Perceive the game screen using the VLM
2. Make intelligent decisions based on the current context
3. Execute actions or action sequences
4. Continue the perceive-plan-act loop autonomously

### Available Modes

- `--agent-auto`: Fully autonomous operation (recommended)
- Manual controls available: Tab=Cycle Mode, Space=Agent Step, M=Show State

### Performance Metrics

When running, you'll see real-time metrics:
- Server FPS: Game execution speed (~60-80 FPS target)
- VLM inference time: ~2.3s for perception processing
- Action queue status and step counts

---

## Model Training & Development

### VLM Perception Training

The perception system uses a fine-tuned Qwen2-VL model. While we currently use the base model for optimal performance, you can retrain with:

```bash
python train_perception_vlm.py \
    --dataset_path data/perception_seed.jsonl \
    --model_id "Qwen/Qwen2-VL-2B-Instruct" \
    --output_dir models/perception_custom
```

### Testing & Validation

```bash
# Test perception system
python test_perception_gpu.py

# Test default configuration
python tests/test_default_config.py
```

---

## Competition Context & Strategy

This agent is being developed for **Track 2 (RPG Speedrunning)** of the NeurIPS 2025 Pok√©Agent Challenge. The primary goal is to achieve the highest **Raw Performance**, measured by milestone completion percentage and efficiency (time and actions).

### New Ranking Criteria (Updated Oct 19th, 2025)
The competition recently clarified that the main leaderboard rankings are determined **solely by objective Raw Performance metrics**. The previously mentioned scaffolding penalty (Œª) has been removed from the main ranking calculation.

### Our Strategy for Success
Our architecture is designed to maximize Raw Performance while also showcasing innovation for the separate **Judges' Choice Awards**:

1.  **Maximize Speed & Milestones:** Our hierarchical planning and RL-based control systems aim to find and execute the fastest possible routes through the game's milestones. The focus is on efficient decision-making and execution.
2.  **Demonstrate Innovation:** While not directly impacting the main rank, our learning-based components (VLM Perception, potential RL Memory Management) showcase novel AI capabilities that align with the spirit of the Judges' Choice awards for innovative methods.
3.  **Methodology Documentation:** We will still thoroughly document our approach, including scaffolding dimensions, to be eligible for the Judges' Choice awards.

### Evaluation Metrics

Submissions are ranked on **Adjusted Performance**, which balances:
- **Raw Performance**: Speed, milestone completion, and efficiency
- **Scaffolding Penalty**: Reduced scores for excessive human-provided knowledge or external tools

### Our Competitive Advantages

Our architecture is designed to maximize Adjusted Performance by:

1. **Minimal Scaffolding**: Uses base VLM models with learned capabilities rather than extensive human-coded knowledge
2. **Autonomous Decision Making**: No human-in-the-loop feedback during execution
3. **Efficient Architecture**: Real-time performance suitable for speedrunning requirements
4. **Robust Operation**: Intelligent fallback systems prevent crashes and ensure progress

### Current Capabilities

- ‚úÖ Stable autonomous operation without human intervention
- ‚úÖ Real-time visual understanding and decision making
- ‚úÖ Context-aware action selection and sequencing
- ‚úÖ Progression through game intro and basic navigation
- üîÑ Advanced strategic planning and optimization (in development)

---

## Documentation

- `docs/ARCHITECTURAL_BLUEPRINT.md` - Detailed technical architecture and design philosophy
- `docs/PROJECT_PLAN.md` - Development timeline and milestones
- `docs/STARTERKIT_README.md` - Original starter kit setup instructions
- `docs/MODEL_COMPATIBILITY_GUIDE.md` - Model configuration and compatibility notes

## Contributing

This project is under active development for the NeurIPS 2025 competition. See the project plan for current development priorities and upcoming milestones.

---

## License

Based on the official Pok√©Agent Challenge starter kit. See `LICENSE` for details.