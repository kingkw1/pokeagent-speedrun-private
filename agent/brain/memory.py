# agent/brain/memory.py
import chromadb
import uuid
import time
import logging
from chromadb.utils import embedding_functions

logger = logging.getLogger(__name__)

class EpisodicMemory:
    """
    Long-term semantic memory using ChromaDB.
    Allows the agent to log events and retrieve relevant context via vector search.
    """
    def __init__(self, db_path="./memory_db"):
        logger.info(f"[Memory] Initializing VectorDB at {db_path}...")
        
        # 1. Initialize Client (Persistent storage)
        self.client = chromadb.PersistentClient(path=db_path)
        
        # 2. Initialize Embedding Function
        # We use 'all-MiniLM-L6-v2': Fast, local, and good for short sentences.
        self.embedding_fn = embedding_functions.SentenceTransformerEmbeddingFunction(
            model_name="all-MiniLM-L6-v2"
        )
        
        # 3. Get or Create Collection
        self.collection = self.client.get_or_create_collection(
            name="game_history",
            embedding_function=self.embedding_fn
        )
        logger.info(f"[Memory] System Online. Items in DB: {self.collection.count()}")

    def log_event(self, text: str, metadata: dict = None):
        """
        Writes a significant event to the VectorDB.
        """
        # Copy to avoid mutating the caller's dict
        metadata = dict(metadata) if metadata else {}
            
        # Ensure timestamp exists
        if "timestamp" not in metadata:
            metadata["timestamp"] = time.time()

        # Add a UUID to ensure every log is unique
        doc_id = str(uuid.uuid4())

        try:
            self.collection.add(
                documents=[text],
                metadatas=[metadata],
                ids=[doc_id]
            )
        except Exception as e:
            logger.error(f"Failed to log event: {e}")

    def retrieve_raw(self, query: str, n_results: int = 3) -> list[dict]:
        """
        Performs a Semantic Search and returns structured results.
        Each dict contains 'text', 'metadata', and 'distance'.
        """
        count = self.collection.count()
        if count == 0:
            return []

        n_results = min(n_results, count)

        results = self.collection.query(
            query_texts=[query],
            n_results=n_results
        )

        documents = results['documents'][0]
        metadatas = results['metadatas'][0]
        distances = results['distances'][0] if results.get('distances') else [None] * len(documents)

        return [
            {"text": doc, "metadata": meta, "distance": dist}
            for doc, meta, dist in zip(documents, metadatas, distances)
        ]

    def retrieve_relevant(self, query: str, n_results: int = 3, max_distance: float | None = None) -> str:
        """
        Performs a Semantic Search.
        Returns a formatted string of the top N relevant memories.

        Args:
            max_distance: If set, drops results whose cosine distance exceeds
                          this threshold (lower = more similar). Typical useful
                          range for MiniLM is 0.8â€“1.3.
        """
        entries = self.retrieve_raw(query, n_results)

        if max_distance is not None:
            entries = [e for e in entries if e["distance"] is not None and e["distance"] <= max_distance]

        if not entries:
            return "No relevant memories found."

        # Format as a bulleted list for the LLM
        context_string = "\n".join([f"- {e['text']}" for e in entries])
        return context_string

    def clear_memory(self):
        """Debug tool to wipe history (Used for fresh runs/demos)"""
        try:
            self.client.delete_collection("game_history")
            self.collection = self.client.get_or_create_collection(
                name="game_history",
                embedding_function=self.embedding_fn
            )
            logger.info("Database Wiped.")
        except Exception as e:
            logger.error(f"Error clearing DB: {e}")